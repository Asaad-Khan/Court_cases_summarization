import streamlit as st
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from razdel import sentenize
import nltk
import re
from bs4 import BeautifulSoup

nltk.download('stopwords')

# Load pre-trained Russian summarization model
model_name = "IlyaGusev/rut5_base_sum_gazeta"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Text cleaning and preprocessing function
def preprocess(text):
    text = BeautifulSoup(text, "html.parser").get_text()
    text = re.sub(r"–ü–†–ò–ì–û–í–û–†|–ò–º–µ–Ω–µ–º –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏", "", text, flags=re.IGNORECASE)
    text = re.sub(r"[^–∞-—è–ê-–Ø—ë–Å0-9\s]", " ", text)
    return re.sub(r'\s+', ' ', text).strip()


import re

# Extract key case details
def extract_case_details(text):
    """
    Extracts key details from the court case text.
    This includes defendant's name, charges, and punishment details.
    """
    sentences = [sent.text for sent in sentenize(text)]
    
    # Search for the defendant's name
    defendant_match = re.search(r'–≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏\s+([–ê-–Ø–Å][–∞-—è—ë]+\s[–ê-–Ø–Å][–∞-—è—ë]+)', text)
    defendant = defendant_match.group(1) if defendant_match else "–ü–æ–¥—Å—É–¥–∏–º—ã–π"

    # Search for the charges (Ugolovny Kodeks reference)
    charges_match = re.findall(r'—Å—Ç\.\s*\d+\s*—á\.\s*\d+', text)  # Extracts "—Å—Ç. 158 —á.2" etc.
    charges = ', '.join(charges_match) if charges_match else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"

    return defendant, charges

# Extract punishment details (years of sentence)
def extract_punishment_info(text):
    sentences = [sent.text for sent in sentenize(text)]
    keywords = ['–Ω–∞–∑–Ω–∞—á–∏–ª –Ω–∞–∫–∞–∑–∞–Ω–∏–µ', '–ø—Ä–∏–≥–æ–≤–æ—Ä–∏–ª', '–Ω–∞–∫–∞–∑–∞–Ω–∏–µ –≤ –≤–∏–¥–µ', '–ª–µ—Ç –ª–∏—à–µ–Ω–∏—è —Å–≤–æ–±–æ–¥—ã', '—É—Å–ª–æ–≤–Ω–æ', '–∫–æ–ª–æ–Ω–∏–∏']
    punishment_sentences = [sentence for sentence in sentences if any(keyword in sentence.lower() for keyword in keywords)]
    
    # Extract only the most relevant sentencing statement
    if punishment_sentences:
        return min(punishment_sentences, key=len)  # Shortest relevant sentence
    return ''

# Optimized summarization function
def summarize_russian(text):
    cleaned_text = preprocess(text)
    
    # Extract key case details
    defendant, charges = extract_case_details(cleaned_text)
    
    # Run transformer summarization
    inputs = tokenizer(
        cleaned_text,
        return_tensors="pt",
        truncation=True,
        max_length=512
    )
    summary_ids = model.generate(
        inputs["input_ids"],
        num_beams=5,
        no_repeat_ngram_size=3,
        length_penalty=2.0,
        max_length=80,
        min_length=20,
        early_stopping=True
    )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    
    # Extract punishment information
    punishment_info = extract_punishment_info(cleaned_text)

    # Ensure summary contains key case details
    final_summary = f"{defendant} –æ–±–≤–∏–Ω—è–µ—Ç—Å—è –ø–æ {charges}. {summary}"
    if punishment_info:
        final_summary += f" {punishment_info}"
    
    return final_summary

 
# Streamlit app configuration
st.set_page_config(page_title="Russian Court Case Summarizer", layout="wide")

# Streamlit UI
st.title("‚öñÔ∏è Russian Court Case Summarizer")

user_text = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç —Å—É–¥–µ–±–Ω–æ–≥–æ –¥–µ–ª–∞:", height=300)

if st.button("–°–æ–∑–¥–∞—Ç—å —Ä–µ–∑—é–º–µ"):
    if user_text.strip():
        with st.spinner('–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—é–º–µ...'):
            summary = summarize_russian(user_text)
        st.subheader("üìå –†–µ–∑—é–º–µ –¥–µ–ª–∞:")
        st.write(summary)
    else:
        st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç —Å—É–¥–µ–±–Ω–æ–≥–æ –¥–µ–ª–∞.")

